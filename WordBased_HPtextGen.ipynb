{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a 1_to_seq problem (needs decoder)<br>\n",
    "Process:<br>\n",
    "1-Load the data<br>\n",
    "2-Preprocess the data (tokenizing punctualtions, lower case except for names, do the split)<br>\n",
    "3-Create dictionary from the words<br>\n",
    "4-Build and train the model<br>\n",
    "5-Generate the new text<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string\n",
    "import warnings\n",
    "import re\n",
    "from tensorflow.contrib import legacy_seq2seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "f = open(\"HarryPotterCh1_SorcererStone.txt\",\"r\") \n",
    "textbook = f.read()\n",
    "#print(textbook)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create our own function for preprocessing, tokenization as well as creating index of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(string):\n",
    "    #Tokenize the punctuations in order to consider them as words\n",
    "    string = string.replace(\"\\n\", \" nextline \")\n",
    "    string = string.replace(\".\", \" periodmark \")\n",
    "    string = string.replace(\":\", \" colonmark \")\n",
    "    string = string.replace(\";\", \" semicolonmark \")\n",
    "    string = string.replace(\",\", \" commamark \")\n",
    "    string = string.replace(\"?\", \" questionmark \")\n",
    "    string = string.replace(\"!\", \" exclamationmark \")\n",
    "    string = string.replace(\"...\", \" 3dots \")\n",
    "    string = string.replace(\"--\", \" 2dashes \")\n",
    "    string = string.replace(\"\\\"\", \" quotemark \")\n",
    "    string = string.replace(\"(\", \" leftparan \")\n",
    "    string = string.replace(\")\", \" rightparan \")\n",
    "    \n",
    "    #Names to remain capitalized\n",
    "    Names = ['Harry', 'Potter', 'James','Jim', 'Abbott','George','Hannah', 'Susan', 'Bones', 'Minerva','McGonagall','Professor',\n",
    "             'Sprout','Malfoy','Draco','Voldemort','Rubeus','Percy','Snape','Weasley','Hagrid','Fred','Scabbers','Hedwig',\n",
    "            'Sirius','Hermione','Granger','Ronald','Peeves','Vernon','Dursley','Mrs.','Mr.','Norris','Argus','Filch','Nick','Charlie','Neville',\n",
    "             'Quirrell','Dumbledore','Filch','Flitwick','McGonagall','McGuffin','Ollivander','Baron','Pomfrey','Gryffindor','Slytherin',\n",
    "             'Ravenclaw','Hufflepuff']\n",
    "\n",
    "    toLower = lambda x: \" \".join( a if a in Names else a.lower()\n",
    "            for a in x.split() )\n",
    "\n",
    "    string= toLower(string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "text=preprocess_text(textbook)\n",
    "#print(text)\n",
    "text_words=text.split()\n",
    "text_len=len(text_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionary from list of words in text\n",
    "def dictionary(words):\n",
    "    #create list of words without their dupications \n",
    "    words=set(words)\n",
    "    #map word to index\n",
    "    indx = {key: i for i, key in enumerate(words)}\n",
    "    return indx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert from index to words\n",
    "def get_by_key_dict(indx_word,words_dict):\n",
    "    for word, indx in words_dict.iteritems():    \n",
    "        if indx == indx_word:\n",
    "            return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wrought-iron': 0,\n",
       " 'both': 2525,\n",
       " 'foul': 14,\n",
       " 'four': 2,\n",
       " 'woods': 3,\n",
       " 'spiders': 4,\n",
       " 'ornate': 22,\n",
       " 'wizardry': 6,\n",
       " 'Ronald': 7,\n",
       " \"fluffy's\": 8,\n",
       " 'lord': 9,\n",
       " 'flicking': 10,\n",
       " 'three-thirty': 11,\n",
       " 'sinking': 12,\n",
       " 'figg': 13,\n",
       " 'yellow': 1,\n",
       " 'bringing': 103,\n",
       " 'disturb': 16,\n",
       " 'basics': 17,\n",
       " 'wooden': 18,\n",
       " 'wednesday': 19,\n",
       " 'specially': 20,\n",
       " 'tired': 21,\n",
       " 'hanging': 5,\n",
       " 'bacon': 23,\n",
       " 'second': 24,\n",
       " 'crisply': 25,\n",
       " 'sailed': 26,\n",
       " 'scraped': 27,\n",
       " 'iron-gray': 28,\n",
       " 'thunder': 29,\n",
       " 'fingers': 30,\n",
       " \"'smatter\": 31,\n",
       " 'pawed': 32,\n",
       " 'galleons': 33,\n",
       " 'Neville': 34,\n",
       " 'hero': 35,\n",
       " '-then': 36,\n",
       " \"norris's\": 37,\n",
       " 'here': 38,\n",
       " 'reported': 39,\n",
       " 'ashen-faced': 40,\n",
       " 'shriek': 41,\n",
       " 'substance': 260,\n",
       " 'climbed': 43,\n",
       " 'reports': 44,\n",
       " \"i'd\": 45,\n",
       " 'transfixed': 46,\n",
       " \"i'm\": 47,\n",
       " 'golden': 48,\n",
       " 'explained': 49,\n",
       " 'brought': 50,\n",
       " 'stern': 51,\n",
       " 'cheating': 52,\n",
       " 'spoke': 53,\n",
       " 'Vernon': 54,\n",
       " 'music': 55,\n",
       " 'therefore': 56,\n",
       " \"wine's\": 57,\n",
       " 'until': 58,\n",
       " 'relax': 59,\n",
       " 'hurt': 60,\n",
       " 'glass': 61,\n",
       " 'tying': 62,\n",
       " \"closer'n\": 63,\n",
       " 'midst': 382,\n",
       " 'hold': 65,\n",
       " 'circumstances': 66,\n",
       " 'locked': 67,\n",
       " 'plunged': 68,\n",
       " 'locker': 69,\n",
       " 'wand': 70,\n",
       " 'dragons': 71,\n",
       " 'caution': 72,\n",
       " 'want': 73,\n",
       " 'groaned': 74,\n",
       " 'damage': 75,\n",
       " 'how': 76,\n",
       " 'hot': 77,\n",
       " 'hop': 78,\n",
       " 'ronnie': 80,\n",
       " 'beauty': 81,\n",
       " 'outlawed': 82,\n",
       " 'wrong': 83,\n",
       " 'destined': 84,\n",
       " 'types': 85,\n",
       " 'wins': 86,\n",
       " 'sickening': 87,\n",
       " 'snowballs': 88,\n",
       " 'keeps': 89,\n",
       " 'wing': 90,\n",
       " 'wind': 91,\n",
       " 'wine': 92,\n",
       " \"'jordan\": 93,\n",
       " 'dreamed': 94,\n",
       " 'whizzing': 95,\n",
       " 'leg-locker': 5196,\n",
       " 'diagonally': 97,\n",
       " 'aargh': 98,\n",
       " 'fir': 99,\n",
       " 'Abbott': 100,\n",
       " 'wickedly': 101,\n",
       " 'fit': 102,\n",
       " 'screaming': 15,\n",
       " 'fix': 630,\n",
       " 'fie': 105,\n",
       " 'hidden': 106,\n",
       " 'admirable': 107,\n",
       " 'easier': 108,\n",
       " 'corridor': 109,\n",
       " \"weasleys'\": 110,\n",
       " 'jinxing': 111,\n",
       " 'sixteen': 112,\n",
       " 'silver': 113,\n",
       " 'towered': 114,\n",
       " 'arrow': 116,\n",
       " 'dumpy': 117,\n",
       " 'loyalties': 118,\n",
       " 'interfering': 119,\n",
       " 'blushed': 120,\n",
       " 'weirdos': 121,\n",
       " 'snakes': 122,\n",
       " 'liquids': 5730,\n",
       " 'spider': 123,\n",
       " \"we'd\": 124,\n",
       " 'knitting': 1040,\n",
       " 'blinded': 126,\n",
       " 'ice-cold': 127,\n",
       " 'centaurs': 128,\n",
       " 're': 129,\n",
       " 'stamping': 130,\n",
       " 'assured': 131,\n",
       " 'pumpkins': 132,\n",
       " 'checked': 2082,\n",
       " 'enormous': 134,\n",
       " 'ate': 135,\n",
       " 'shelves': 136,\n",
       " 'cauldrons': 137,\n",
       " 'scruff': 138,\n",
       " 'bated': 139,\n",
       " 'speedy': 140,\n",
       " 'potions': 141,\n",
       " 'speeds': 142,\n",
       " 'purpose': 4162,\n",
       " 'playfully': 144,\n",
       " 'wash': 145,\n",
       " 'spinning': 146,\n",
       " 'wasn': 147,\n",
       " 'bitten': 148,\n",
       " 'basketball': 149,\n",
       " 'service': 150,\n",
       " 'scared-looking': 151,\n",
       " 'needed': 152,\n",
       " 'master': 153,\n",
       " 'legs': 154,\n",
       " 'bitter': 155,\n",
       " 'rummaged': 156,\n",
       " 'listen': 157,\n",
       " 'phoenixes': 158,\n",
       " 'frowned': 159,\n",
       " 'popkin': 160,\n",
       " 'motionless': 161,\n",
       " 'lunged': 162,\n",
       " 'positively': 163,\n",
       " 'showed': 164,\n",
       " 'coward': 165,\n",
       " 'tree': 166,\n",
       " 'whinging': 167,\n",
       " 'idly': 168,\n",
       " 'bodyguards': 169,\n",
       " 'feeling': 170,\n",
       " 'keeper': 3392,\n",
       " 'doi*': 172,\n",
       " 'well-': 173,\n",
       " 'dozen': 174,\n",
       " 'affairs': 175,\n",
       " 'escalator': 176,\n",
       " 'concrete': 1048,\n",
       " 'responsible': 178,\n",
       " 'eagerly': 179,\n",
       " 'doors': 180,\n",
       " 'grips': 181,\n",
       " 'committing': 182,\n",
       " 'duddydums': 183,\n",
       " 'shall': 184,\n",
       " 'object': 185,\n",
       " 'looming': 186,\n",
       " 'toil': 187,\n",
       " 'hushing': 188,\n",
       " 'mouth': 189,\n",
       " 'letter': 190,\n",
       " 'erised': 191,\n",
       " 'pearly-white': 192,\n",
       " 'sweetums': 193,\n",
       " 'nettle': 194,\n",
       " 'put-outer': 195,\n",
       " 'scream': 196,\n",
       " 'came': 197,\n",
       " 'saying': 198,\n",
       " 'lessons': 199,\n",
       " 'touches': 200,\n",
       " 'busy': 201,\n",
       " 'clicked': 202,\n",
       " 'shuffling': 203,\n",
       " 'bush': 204,\n",
       " 'touched': 205,\n",
       " 'rich': 206,\n",
       " 'rice': 207,\n",
       " \"sayin'\": 1239,\n",
       " 'wide': 209,\n",
       " 'stammered': 210,\n",
       " 'firenze': 211,\n",
       " 'pocket': 212,\n",
       " 'cushion': 213,\n",
       " 'tips': 3073,\n",
       " 'spilling': 215,\n",
       " 'tights': 216,\n",
       " 'patch': 217,\n",
       " 'you-know-what': 218,\n",
       " 'lurched': 219,\n",
       " 'flanked': 220,\n",
       " 'boarded': 221,\n",
       " 'blew': 222,\n",
       " 'fair': 223,\n",
       " 'fail': 224,\n",
       " 'hammer': 225,\n",
       " 'best': 226,\n",
       " 'lots': 227,\n",
       " 'stamps': 228,\n",
       " \"book's\": 229,\n",
       " 'claws': 230,\n",
       " 'never': 3077,\n",
       " 'rolled': 232,\n",
       " 'smelled': 233,\n",
       " 'paddington': 234,\n",
       " 'debt': 235,\n",
       " 'pity': 236,\n",
       " 'accident': 237,\n",
       " 'country': 238,\n",
       " 'askew': 239,\n",
       " 'vomitflavored': 240,\n",
       " 'planned': 241,\n",
       " 'logic': 3719,\n",
       " 'argue': 243,\n",
       " 'tinge': 244,\n",
       " 'asked': 245,\n",
       " 'vain': 246,\n",
       " 'gleaming': 247,\n",
       " 'blonde': 248,\n",
       " 'summat': 249,\n",
       " 'trapdoor': 250,\n",
       " 'breathlessly': 251,\n",
       " 'much': 252,\n",
       " 'chucked': 253,\n",
       " 'stadium': 254,\n",
       " 'tallest': 255,\n",
       " 'life': 256,\n",
       " 'malfoys': 257,\n",
       " 'snap': 258,\n",
       " 'dota': 259,\n",
       " 'kids': 42,\n",
       " 'child': 261,\n",
       " 'worked': 262,\n",
       " 'flooding': 263,\n",
       " 'remembering': 264,\n",
       " 'doormat': 265,\n",
       " 'played': 266,\n",
       " 'player': 267,\n",
       " 'trusted': 268,\n",
       " 'damaged': 269,\n",
       " 'taped': 3087,\n",
       " 'things': 271,\n",
       " 'swarthy': 272,\n",
       " 'split': 273,\n",
       " 'boiled': 274,\n",
       " 'remembrall': 275,\n",
       " 'marched': 276,\n",
       " 'tune': 277,\n",
       " 'echoed': 278,\n",
       " 'thing*': 279,\n",
       " 'echoes': 280,\n",
       " 'sleeps': 281,\n",
       " 'sleepy': 282,\n",
       " 'spectacles': 283,\n",
       " 'ham': 284,\n",
       " 'old-fashioned': 285,\n",
       " \"goblin's\": 286,\n",
       " 'had': 287,\n",
       " 'hag': 288,\n",
       " 'innocent': 1781,\n",
       " 'has': 290,\n",
       " 'hat': 291,\n",
       " 'unblinkingly': 292,\n",
       " 'casually': 293,\n",
       " 'possible': 294,\n",
       " 'possibly': 295,\n",
       " 'sleep-': 296,\n",
       " 'shadow': 297,\n",
       " 'bushy': 298,\n",
       " 'desire': 299,\n",
       " 'remind': 300,\n",
       " 'pavement': 301,\n",
       " 'steps': 302,\n",
       " 'meringue': 303,\n",
       " \"constrictor's\": 304,\n",
       " 'right': 305,\n",
       " 'old': 306,\n",
       " 'drowned': 3092,\n",
       " 'crowd': 308,\n",
       " 'people': 309,\n",
       " 'crown': 310,\n",
       " \"an'\": 5233,\n",
       " 'creep': 312,\n",
       " 'enemies': 313,\n",
       " 'gasps': 314,\n",
       " \"daddy's\": 315,\n",
       " 'ruffled': 316,\n",
       " 'for': 317,\n",
       " 'bottom': 318,\n",
       " 'pinched': 319,\n",
       " 'substitutes': 320,\n",
       " 'losing': 321,\n",
       " 'shaken': 322,\n",
       " 'visitors': 323,\n",
       " 'riffraff': 324,\n",
       " 'stoked': 325,\n",
       " 'festoons': 326,\n",
       " 'slightly': 327,\n",
       " 'meddle': 328,\n",
       " 'raised': 329,\n",
       " 'sob': 330,\n",
       " 'son': 331,\n",
       " \"potter's\": 332,\n",
       " 'thankful': 333,\n",
       " 'beings': 334,\n",
       " 'half-and-half': 335,\n",
       " 'tailcoats': 336,\n",
       " 'tame': 337,\n",
       " 'grasping': 338,\n",
       " \"boy's\": 339,\n",
       " \"minute's\": 340,\n",
       " 'greatness': 341,\n",
       " 'overhead': 3099,\n",
       " 'happy': 343,\n",
       " 'back': 344,\n",
       " 'shrilly': 345,\n",
       " 'peppermints': 346,\n",
       " 's-s-sorry': 347,\n",
       " 'beech': 348,\n",
       " 'duel': 349,\n",
       " \"else's\": 350,\n",
       " 'inside': 351,\n",
       " 'smashed': 352,\n",
       " 'panels': 353,\n",
       " 'feet': 5591,\n",
       " 'later': 5240,\n",
       " 'bathrobe': 355,\n",
       " 'soothe': 2183,\n",
       " 'sprouts': 357,\n",
       " \"myst'ry\": 358,\n",
       " 'dealer': 359,\n",
       " 'crutches': 360,\n",
       " 'dotted': 361,\n",
       " 'floor': 362,\n",
       " 'smell': 363,\n",
       " 'roll': 364,\n",
       " \"'t\": 365,\n",
       " 'palms': 366,\n",
       " 'devon': 367,\n",
       " 'miranda': 368,\n",
       " 'rolling': 369,\n",
       " '-harry': 3106,\n",
       " 'hygienic': 372,\n",
       " 'fastened': 373,\n",
       " 'beamed': 374,\n",
       " 'time': 375,\n",
       " 'push': 376,\n",
       " 'gown': 377,\n",
       " 'whoever': 378,\n",
       " 'blacks': 379,\n",
       " '90': 380,\n",
       " 'chair': 381,\n",
       " 'hole': 64,\n",
       " 'foribidden': 383,\n",
       " \"when's\": 384,\n",
       " 'gorgons': 385,\n",
       " 'behave': 1087,\n",
       " 'icing': 387,\n",
       " 'jerk': 388,\n",
       " 'choice': 389,\n",
       " \"bott's\": 390,\n",
       " 'gloomy': 391,\n",
       " 'right-handed': 392,\n",
       " 'exact': 393,\n",
       " 'minute': 394,\n",
       " 'getups': 395,\n",
       " '1709': 396,\n",
       " 'leave': 397,\n",
       " 'team': 399,\n",
       " 'loads': 400,\n",
       " 'bonfire': 401,\n",
       " 'sixty-five': 402,\n",
       " 'prevent': 403,\n",
       " 'sigh': 404,\n",
       " 'sign': 405,\n",
       " \"'you-\": 406,\n",
       " 'crackpot': 407,\n",
       " 'melt': 408,\n",
       " 'lazily': 409,\n",
       " 'falling': 410,\n",
       " 'crashing': 4299,\n",
       " 'badge': 412,\n",
       " 'canary-yellow': 413,\n",
       " 'funeral': 414,\n",
       " 'address': 415,\n",
       " 'alone': 416,\n",
       " 'along': 417,\n",
       " 'twelve-foot': 418,\n",
       " 'hurtling': 419,\n",
       " 'dusty': 2561,\n",
       " \"second's\": 421,\n",
       " 'wherever': 422,\n",
       " 'carpets': 423,\n",
       " 'love': 424,\n",
       " 'radish': 425,\n",
       " 'prefer': 426,\n",
       " 'bloody': 427,\n",
       " 'shields': 4456,\n",
       " 'crammed': 429,\n",
       " 'toppled': 430,\n",
       " 'working': 431,\n",
       " 'positive': 2626,\n",
       " 'angry': 433,\n",
       " 'tightly': 434,\n",
       " 'wondering': 435,\n",
       " 'broken-down': 436,\n",
       " 'wicked': 437,\n",
       " 'marbles': 438,\n",
       " 'afford': 439,\n",
       " 'everywhere': 440,\n",
       " 'riders': 441,\n",
       " 'anything': 1093,\n",
       " 'jewel-bright': 443,\n",
       " 'pretend': 444,\n",
       " 'aaaaaaaaaargh': 445,\n",
       " 'believes': 446,\n",
       " 'believed': 447,\n",
       " 'toffee': 448,\n",
       " 'admired': 450,\n",
       " 'frogs': 451,\n",
       " 'broomshed': 452,\n",
       " 'hides': 453,\n",
       " 'allowed': 454,\n",
       " 'stole': 455,\n",
       " 'winter': 456,\n",
       " 'buttered': 457,\n",
       " 'poking': 458,\n",
       " 'elephant': 459,\n",
       " 'spot': 460,\n",
       " 'date': 461,\n",
       " 'such': 462,\n",
       " 'filthy': 463,\n",
       " 'lids': 464,\n",
       " 'natural': 465,\n",
       " 'st': 466,\n",
       " 'darkened': 467,\n",
       " 'so': 468,\n",
       " 'swollen': 469,\n",
       " 'pulled': 470,\n",
       " 'wastepaper': 471,\n",
       " 'years': 472,\n",
       " 'course': 473,\n",
       " 'bucking': 474,\n",
       " 'tore': 475,\n",
       " 'jig': 476,\n",
       " 'yawning': 477,\n",
       " 'nearsighted': 479,\n",
       " 'paraded': 480,\n",
       " 'torn': 481,\n",
       " 'suspicion': 482,\n",
       " 'thump': 483,\n",
       " 'suspension': 484,\n",
       " 'instantly': 486,\n",
       " 'matches': 487,\n",
       " 'smarter': 488,\n",
       " 'smarten': 489,\n",
       " 'arriving': 490,\n",
       " 'sorted': 491,\n",
       " \"yeh've\": 492,\n",
       " 'shhh': 493,\n",
       " 'chappie': 494,\n",
       " 'shouted': 495,\n",
       " 'bowling': 496,\n",
       " 'veins': 497,\n",
       " 'quarter': 498,\n",
       " 'repaired': 499,\n",
       " 'square': 500,\n",
       " 'bursting': 501,\n",
       " 'n-not': 502,\n",
       " 'gloomy-looking': 503,\n",
       " 'entering': 504,\n",
       " 'beetle': 505,\n",
       " 'troll': 506,\n",
       " 'seriously': 507,\n",
       " 'exploding': 508,\n",
       " 'hufflepuff': 509,\n",
       " 'facedown': 510,\n",
       " 'seventh': 511,\n",
       " 'quite': 512,\n",
       " 'complicated': 513,\n",
       " 'seventy': 3098,\n",
       " 'training': 515,\n",
       " 'dunno': 516,\n",
       " 'massive': 3122,\n",
       " 'hooch': 518,\n",
       " 'saving': 519,\n",
       " 'spoken': 520,\n",
       " 'potter': 521,\n",
       " \"meddlin'\": 522,\n",
       " 'one': 523,\n",
       " 'open': 524,\n",
       " 'dursleys': 525,\n",
       " 'ripping': 526,\n",
       " 'city': 527,\n",
       " 'bite': 528,\n",
       " 'seventy-': 529,\n",
       " 'uric': 530,\n",
       " '2': 531,\n",
       " 'stuffed': 532,\n",
       " 'bits': 533,\n",
       " 'owl-free': 534,\n",
       " 'lingering': 535,\n",
       " 'proving': 536,\n",
       " 'meddling': 537,\n",
       " 'snatch': 538,\n",
       " 'ridiculous': 539,\n",
       " 'surged': 540,\n",
       " 'pheasants': 541,\n",
       " 'depressed': 542,\n",
       " 'favors': 543,\n",
       " 'coats': 544,\n",
       " 'future': 545,\n",
       " 'wandering': 546,\n",
       " 'tasted': 547,\n",
       " 'nicer': 548,\n",
       " 'round-faced': 549,\n",
       " 'turned': 550,\n",
       " 'afternoons': 3299,\n",
       " 'alley': 3301,\n",
       " 'sad': 553,\n",
       " 'say': 554,\n",
       " 'buried': 555,\n",
       " 'lurking': 556,\n",
       " 'saw': 557,\n",
       " 'sat': 558,\n",
       " 'nott': 559,\n",
       " 'aside': 560,\n",
       " 'zoo': 561,\n",
       " 'note': 562,\n",
       " 'take': 563,\n",
       " 'wanting': 564,\n",
       " 'blinding': 565,\n",
       " 'atop': 1116,\n",
       " 'that-that': 567,\n",
       " \"teacher's\": 568,\n",
       " 'handing': 569,\n",
       " \"c'mere\": 570,\n",
       " 'opposite': 571,\n",
       " 'syllable': 572,\n",
       " 'knew': 573,\n",
       " 'knee': 574,\n",
       " 'pages': 575,\n",
       " 'lawn': 576,\n",
       " 'drive': 577,\n",
       " 'crooked': 578,\n",
       " \"wantin'\": 579,\n",
       " 'trembled': 580,\n",
       " 'laws': 581,\n",
       " 'walking': 582,\n",
       " \"d-d-don't\": 583,\n",
       " 'bright': 584,\n",
       " 'imagined': 585,\n",
       " 'slot': 586,\n",
       " 'merlin': 587,\n",
       " 'cloak': 588,\n",
       " 'tears': 589,\n",
       " 'going': 590,\n",
       " \"bidin'\": 591,\n",
       " 'outrage': 592,\n",
       " 'robe': 593,\n",
       " 'guarded': 594,\n",
       " 'assistant': 595,\n",
       " 'freezing': 596,\n",
       " 'cursing': 597,\n",
       " 'awaiting': 598,\n",
       " 'hinges': 599,\n",
       " 'saliva': 600,\n",
       " 'borrow': 601,\n",
       " 'worried': 602,\n",
       " \"goin'\": 603,\n",
       " \"could've\": 604,\n",
       " 'bombs': 605,\n",
       " 'where': 606,\n",
       " 'vision': 607,\n",
       " 'arrival': 608,\n",
       " 'raged': 609,\n",
       " 'dived': 610,\n",
       " 'horned': 1121,\n",
       " 'dives': 612,\n",
       " 'alarming': 5258,\n",
       " 'dormitory': 614,\n",
       " 'moldy': 615,\n",
       " 'moons': 616,\n",
       " 'enjoys': 617,\n",
       " 'youknow-who': 618,\n",
       " \"hermione's\": 619,\n",
       " 'Malfoy': 620,\n",
       " 'concentrate': 621,\n",
       " \"you-know-who's\": 622,\n",
       " 'many': 623,\n",
       " 'thickly': 624,\n",
       " 'flipped': 625,\n",
       " 's': 626,\n",
       " 'sacked': 1900,\n",
       " 'mane': 628,\n",
       " 'expression': 629,\n",
       " 'backpack': 104,\n",
       " \"can't\": 631,\n",
       " 'twin': 632,\n",
       " 'twig': 633,\n",
       " 'boat': 634,\n",
       " 'math': 4382,\n",
       " 'teddy': 635,\n",
       " 'stretch': 636,\n",
       " 'west': 637,\n",
       " 'vacation': 638,\n",
       " 'breath': 639,\n",
       " 'wants': 640,\n",
       " 'cliodna': 641,\n",
       " 'lanes': 642,\n",
       " 'thousand': 643,\n",
       " 'photos': 644,\n",
       " 'tightened': 645,\n",
       " 'queasy': 646,\n",
       " 'ghoulie': 647,\n",
       " 'voldemort': 648,\n",
       " 'squeezed': 3899,\n",
       " 'passes': 5593,\n",
       " 'snuffbox': 650,\n",
       " 'threequarters': 651,\n",
       " 'limping': 652,\n",
       " 'ivy': 653,\n",
       " 'rooting': 654,\n",
       " \"-i'm\": 655,\n",
       " 'strange-looking': 656,\n",
       " 'edged': 657,\n",
       " \"figure's\": 658,\n",
       " 'cheers': 659,\n",
       " 'edges': 660,\n",
       " 'cheery': 661,\n",
       " 'flocks': 662,\n",
       " 'pacing': 663,\n",
       " 'pewter': 3013,\n",
       " 'summer': 665,\n",
       " 'being': 666,\n",
       " 'rest': 667,\n",
       " 'slimy': 668,\n",
       " 'fateful': 669,\n",
       " 'snarled': 670,\n",
       " 'weekly': 671,\n",
       " 'bricks': 5291,\n",
       " \"diggle's\": 673,\n",
       " 'skies': 674,\n",
       " 'starving': 675,\n",
       " 'around': 676,\n",
       " \"bein'\": 677,\n",
       " 'knobbly': 678,\n",
       " 'dark': 679,\n",
       " 'traffic': 680,\n",
       " 'insist': 5294,\n",
       " 'vacuum': 682,\n",
       " 'world': 683,\n",
       " 'snare': 684,\n",
       " 'dare': 685,\n",
       " 'stranger': 686,\n",
       " 'claw': 687,\n",
       " 'sorcerer': 1135,\n",
       " 'clap': 689,\n",
       " 'seating': 690,\n",
       " \"couldn't\": 2166,\n",
       " 'pickled': 692,\n",
       " 'thinks': 693,\n",
       " \"'scuse\": 694,\n",
       " 'revenges': 695,\n",
       " 'dimpled': 696,\n",
       " \"payin'\": 697,\n",
       " 'biased': 698,\n",
       " 'power': 699,\n",
       " 'forgetfulness': 700,\n",
       " 'woodcroft': 701,\n",
       " 'ducking': 702,\n",
       " 'stone': 704,\n",
       " 'package': 705,\n",
       " 'favorite': 706,\n",
       " 'slender': 707,\n",
       " 'side': 3153,\n",
       " 'act': 709,\n",
       " 'mean': 3154,\n",
       " 'nastily': 711,\n",
       " 'chivalry': 712,\n",
       " 'curling': 713,\n",
       " 'burning': 714,\n",
       " 'image': 715,\n",
       " 'legged': 716,\n",
       " 'sneering': 717,\n",
       " 'parties': 718,\n",
       " \"buyin'\": 719,\n",
       " 'telescope': 720,\n",
       " \"how's\": 721,\n",
       " 'her': 4389,\n",
       " 'sloped': 723,\n",
       " 'sides': 1142,\n",
       " 'brazilian': 725,\n",
       " 'hem': 726,\n",
       " 'ago': 1143,\n",
       " 'complete': 728,\n",
       " 'n-nothing': 729,\n",
       " 'gliding': 730,\n",
       " 'foreheads': 731,\n",
       " 'Harry': 732,\n",
       " 'mice': 733,\n",
       " 'with': 734,\n",
       " 'buying': 735,\n",
       " 'handsome': 736,\n",
       " \"they're\": 737,\n",
       " 'rush': 738,\n",
       " 'rage': 739,\n",
       " 'tripe': 740,\n",
       " 'rags': 741,\n",
       " 'dirty': 742,\n",
       " 'quotemark': 743,\n",
       " 'he-': 744,\n",
       " 'agree': 745,\n",
       " 'gone': 746,\n",
       " 'fright': 747,\n",
       " 'exhausted': 748,\n",
       " 'certain': 749,\n",
       " 'am': 750,\n",
       " 'an': 751,\n",
       " 'wildly': 4809,\n",
       " 'as': 752,\n",
       " 'carved': 753,\n",
       " 'fumbling': 754,\n",
       " 'watched': 755,\n",
       " 'tremble': 756,\n",
       " 'cream': 757,\n",
       " 'beaming': 758,\n",
       " 'drafts': 759,\n",
       " 'drafty': 760,\n",
       " 'tight': 761,\n",
       " 'beaks': 762,\n",
       " 'waving': 763,\n",
       " 'herbs': 764,\n",
       " 'snitch': 765,\n",
       " 'Flitwick': 766,\n",
       " 'tricky': 767,\n",
       " 'sobbing': 768,\n",
       " 'tricks': 769,\n",
       " 'hello': 1148,\n",
       " 'hushed': 3163,\n",
       " 'birdcage': 772,\n",
       " 'beware': 773,\n",
       " 'code': 774,\n",
       " 'hunting': 775,\n",
       " 'laughing': 4210,\n",
       " 'to': 776,\n",
       " 'tail': 777,\n",
       " 'chewing': 778,\n",
       " 'th': 779,\n",
       " 'smile': 780,\n",
       " 'sc-': 781,\n",
       " 'case': 5307,\n",
       " 'returned': 783,\n",
       " 'detention': 784,\n",
       " 'floated': 785,\n",
       " 'dodged': 4775,\n",
       " 'large': 787,\n",
       " 'harry': 788,\n",
       " 'small': 789,\n",
       " 'dodges': 790,\n",
       " 'sank': 791,\n",
       " 'quicker': 792,\n",
       " 'past': 793,\n",
       " 'carriages': 794,\n",
       " 'pass': 795,\n",
       " 'clock': 796,\n",
       " 'poisonous': 797,\n",
       " 'whirled': 798,\n",
       " 'nurse': 799,\n",
       " 'full': 800,\n",
       " 'escaping': 801,\n",
       " \"lecturin'\": 802,\n",
       " 'Charlie': 803,\n",
       " 'leaping': 804,\n",
       " 'hours': 805,\n",
       " 'november': 806,\n",
       " 'legend': 807,\n",
       " 'cock-and-bull': 4921,\n",
       " 'windowsill': 1157,\n",
       " 'experience': 810,\n",
       " 'pick': 811,\n",
       " 'smuggle': 812,\n",
       " 'rummaging': 813,\n",
       " 'gawking': 814,\n",
       " 'followed': 815,\n",
       " 'hook-nosed': 816,\n",
       " 'indoors': 817,\n",
       " 'bagshot': 818,\n",
       " 'more': 819,\n",
       " 'door': 820,\n",
       " 'company': 821,\n",
       " 'spoils': 822,\n",
       " 'doom': 823,\n",
       " 'cunning': 824,\n",
       " 'sizing': 825,\n",
       " 'keeping': 826,\n",
       " 'snuffling': 2188,\n",
       " 'science': 828,\n",
       " 'stalagmite': 829,\n",
       " \"fang's\": 830,\n",
       " 'learn': 831,\n",
       " 'knocked': 832,\n",
       " 'beautiful': 833,\n",
       " 'accept': 834,\n",
       " 'flitwicles': 835,\n",
       " 'fiercely': 836,\n",
       " 'sense': 837,\n",
       " 'disliked': 5134,\n",
       " 'dress': 839,\n",
       " 'tapestry': 840,\n",
       " 'huge': 5140,\n",
       " 'glowing': 842,\n",
       " 'creature': 843,\n",
       " 'waved': 844,\n",
       " 'plant': 845,\n",
       " \"hagrid's\": 846,\n",
       " 'countryside': 847,\n",
       " 'mended': 848,\n",
       " 'director': 5328,\n",
       " 'waves': 849,\n",
       " 'pleaded': 850,\n",
       " 'flutter': 851,\n",
       " 'refuse': 852,\n",
       " 'twisting': 853,\n",
       " 'tentacles': 854,\n",
       " 'deafening': 855,\n",
       " 'helplessly': 856,\n",
       " 'replied': 857,\n",
       " 'rocketed': 5248,\n",
       " 'bowl': 859,\n",
       " 'paper': 860,\n",
       " 'brim': 861,\n",
       " 'signs': 862,\n",
       " 'smiling': 863,\n",
       " 'its': 864,\n",
       " 'roots': 865,\n",
       " 'rapidly': 866,\n",
       " 'sauce': 867,\n",
       " 'goggle': 3417,\n",
       " \"it'\": 869,\n",
       " 'snowball': 870,\n",
       " 'weeds': 871,\n",
       " 'sucked': 872,\n",
       " 'stewed': 873,\n",
       " 'always': 874,\n",
       " 'all-': 875,\n",
       " 'isle': 5325,\n",
       " 'found': 877,\n",
       " 'england': 878,\n",
       " 'upstairs': 879,\n",
       " 'bendy': 880,\n",
       " 'really': 881,\n",
       " 'try': 1177,\n",
       " 'missed': 883,\n",
       " 'candlelight': 884,\n",
       " 'misses': 885,\n",
       " 'levi-o-sa': 3186,\n",
       " 'darling': 887,\n",
       " 'plotting': 888,\n",
       " 'highway': 889,\n",
       " 'nervously': 890,\n",
       " 'bungler': 891,\n",
       " 'drifting': 892,\n",
       " 'murmur': 893,\n",
       " 'imagine': 894,\n",
       " 'bumped': 895,\n",
       " 'pairs': 896,\n",
       " 'reared': 897,\n",
       " 'castle': 898,\n",
       " \"door's\": 899,\n",
       " 'gazed': 900,\n",
       " 'rooted': 901,\n",
       " 'slipped': 902,\n",
       " 'wilder': 903,\n",
       " 'girls': 3189,\n",
       " 'number': 905,\n",
       " 'murmured': 906,\n",
       " 'guess': 907,\n",
       " 'heads': 908,\n",
       " 'jet': 909,\n",
       " 'swipe': 910,\n",
       " 'threatening': 911,\n",
       " 'flimsy': 912,\n",
       " 'forevermore': 913,\n",
       " 'erupted': 914,\n",
       " 'codswallop': 915,\n",
       " 'rain': 4790,\n",
       " 'stairs': 916,\n",
       " 'determined': 917,\n",
       " 'remembers': 918,\n",
       " \"wizardin'\": 919,\n",
       " 'mossy': 920,\n",
       " 'man-crushing': 921,\n",
       " 'guarding': 922,\n",
       " 'blond': 923,\n",
       " 'cleverness': 924,\n",
       " 'sell': 925,\n",
       " 'scrawled': 926,\n",
       " 'Hannah': 927,\n",
       " 'odd': 928,\n",
       " 'silvery': 929,\n",
       " 'also': 930,\n",
       " \"hour's\": 931,\n",
       " 'shops': 5314,\n",
       " 'play': 933,\n",
       " 'firsthand': 934,\n",
       " 'blackboard': 935,\n",
       " 'swiftly': 936,\n",
       " 'unused': 937,\n",
       " \"guardin'\": 938,\n",
       " 'plan': 939,\n",
       " 'bustled': 940,\n",
       " \"baron'll\": 941,\n",
       " 'singled': 942,\n",
       " 'pliable': 943,\n",
       " 'seize': 944,\n",
       " 'sometimes': 945,\n",
       " 'cover': 946,\n",
       " 'barred': 947,\n",
       " 'donkeys': 948,\n",
       " 'dragged': 949,\n",
       " 'golf': 950,\n",
       " 'gold': 951,\n",
       " 'session': 953,\n",
       " \"frightenin'\": 954,\n",
       " 'writer': 955,\n",
       " 'failed': 956,\n",
       " 'columns': 957,\n",
       " 'begins': 3199,\n",
       " 'downpour': 959,\n",
       " 'sunny': 960,\n",
       " \"they'd\": 961,\n",
       " 'periodmark': 962,\n",
       " 'frightening': 963,\n",
       " 'enemy': 964,\n",
       " 'sleeve': 965,\n",
       " 'tabby': 966,\n",
       " 'tottering': 967,\n",
       " \"knowin'\": 968,\n",
       " 'body': 969,\n",
       " 'delicately': 970,\n",
       " 'set': 971,\n",
       " \"'gar'\": 972,\n",
       " 'nibble': 973,\n",
       " 'grubby-looking': 974,\n",
       " 'see': 975,\n",
       " 'sec': 976,\n",
       " 'sea': 977,\n",
       " 'knees': 978,\n",
       " 'spirits': 979,\n",
       " 'currently': 980,\n",
       " 'topmost': 981,\n",
       " \"las'\": 983,\n",
       " 'nope': 984,\n",
       " 'beasts': 4286,\n",
       " 'knowing': 986,\n",
       " 'screeched': 987,\n",
       " 'untouched': 988,\n",
       " 'hooted': 989,\n",
       " \"good's\": 990,\n",
       " 'last': 991,\n",
       " 'barely': 992,\n",
       " \"'s'\": 993,\n",
       " \"we've\": 5351,\n",
       " 'approve': 995,\n",
       " 'load': 996,\n",
       " \"stone's\": 997,\n",
       " \"everyone's\": 998,\n",
       " 'drunk': 2825,\n",
       " 'acted': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_index=dictionary(text_words)\n",
    "words_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create sequences of 10 length (given 10 words as inputs, predict 1 word for output added to the previos words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  create_model_inputs(batch_size):\n",
    "    '''Define model inputs'''\n",
    "    \n",
    "    #Model's placeholders for inputs\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    return inputs,targets,keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embeddings(inputs,vocabulary_size,embedding_size):\n",
    "    '''Intialize embeddings for the words. Embedding layer connects the words to the LSTM layers (words are embedded to the embedding_size vectors instead of vocabulary size vectors or one hot vectors which aren't efficient). Here, we used random_uniform distribution to initialize the words' embeddings and then they are trained by the model to have more meaningful embeddings'''\n",
    "    \n",
    "    #Embedding Layer\n",
    "    embedding = tf.Variable(tf.random_uniform((vocabulary_size, embedding_size), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs)\n",
    "    \n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def  build_RNN(inputs,num_hidden,lstm_layer_numbers,keep_prob,batch_size):\n",
    "    '''Build RNN'''\n",
    "\n",
    "    #Define LSTM layers\n",
    "    lstms=[]\n",
    "    for i in range(lstm_layer_numbers):\n",
    "        lstms.append(tf.contrib.rnn.BasicLSTMCell(num_hidden))\n",
    "    # Add regularization dropout to the LSTM cells\n",
    "    drops = [tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob) for lstm in lstms]\n",
    "    # Stack up multiple LSTM layers\n",
    "    stacked_lstm = tf.contrib.rnn.MultiRNNCell(drops)\n",
    "    # Getting the initial state\n",
    "    initial_state = stacked_lstm.zero_state(batch_size, tf.float32)    \n",
    "\n",
    "    return initial_state, stacked_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(text_words,text_len,seq_len, batch_size,number_of_words_in_one_batch,n_batches):\n",
    "    '''Using generator to return batches'''\n",
    "    \n",
    "    #This makes the input data to be compatible with seq_len\n",
    "    text_all_batches = text_words[:n_batches*number_of_words_in_one_batch]\n",
    "    index_text_all_batches=[]\n",
    "    for i in text_all_batches:\n",
    "        if i in words_index:\n",
    "            index_text_all_batches.append(words_index[i])\n",
    "        \n",
    "    #index_text_all_batches={v for k,v in words_index.items() if k in text_all_batches}\n",
    "    #get word index for words for batches\n",
    "    input_seq=list(index_text_all_batches)\n",
    "    output_seq=input_seq\n",
    "    output_seq.append(output_seq.pop(output_seq[0]))\n",
    "    for ii in range(0, len(text_all_batches), number_of_words_in_one_batch):\n",
    "        yield input_seq[ii:ii+number_of_words_in_one_batch], output_seq[ii:ii+number_of_words_in_one_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Parameters\n",
    "# number of units\n",
    "n_input= len(words_index)\n",
    "num_hidden = 512\n",
    "lstm_layer_numbers=3\n",
    "embed_size=512\n",
    "batch_size= 512\n",
    "learning_rate=0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a graph for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 512, 6220)\n",
      "('epoch: ', 0)\n",
      "('cost_train=', 6.8164841413497923)\n",
      "('epoch: ', 200)\n",
      "('cost_train=', 4.2910677194595346)\n",
      "('epoch: ', 400)\n",
      "('cost_train=', 0.76123638451099396)\n",
      "('epoch: ', 600)\n",
      "('cost_train=', 0.2490734979510307)\n",
      "('epoch: ', 800)\n",
      "('cost_train=', 0.15294892862439155)\n",
      "('epoch: ', 1000)\n",
      "('cost_train=', 0.12359673902392387)\n",
      "('epoch: ', 1200)\n",
      "('cost_train=', 0.10449819341301915)\n",
      "('epoch: ', 1400)\n",
      "('cost_train=', 0.092972404509782794)\n",
      "('epoch: ', 1600)\n",
      "('cost_train=', 0.083048222213983544)\n",
      "('epoch: ', 1800)\n",
      "('cost_train=', 0.079184707440435875)\n",
      "('epoch: ', 2000)\n",
      "('cost_train=', 0.071434586308896542)\n"
     ]
    }
   ],
   "source": [
    "graph0 = tf.Graph()\n",
    "#There exits a global default graph created by tenserflow, for new graphs we need to set them as a default graph\n",
    "with graph0.as_default():\n",
    "    inputs,targets,keep_prob=create_model_inputs(batch_size)\n",
    "    \n",
    "    #tf.AUTO_REUSE for reuisng the same scope for generating as for traning\n",
    "    with tf.variable_scope('rnn1', reuse=tf.AUTO_REUSE):\n",
    "        embeds=build_embeddings(inputs,n_input,embed_size)\n",
    "        initial_state, stacked_lstm_cells = build_RNN(inputs,num_hidden,lstm_layer_numbers,keep_prob,batch_size)\n",
    "        #need to unstack the sequence of input into a list of tensors\n",
    "        seq_input = [tf.squeeze(i,[1]) for i in tf.split(embeds,seq_len,1)] \n",
    "        outputs, final_state = legacy_seq2seq.rnn_decoder(seq_input, initial_state, stacked_lstm_cells, loop_function=None)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    logits = tf.contrib.layers.fully_connected(outputs, n_input, activation_fn=None)\n",
    "    \n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "    print(probs.shape)\n",
    "    cost =  tf.contrib.seq2seq.sequence_loss(\n",
    "            logits,\n",
    "            targets,\n",
    "            tf.ones([batch_size, (seq_len)])    \n",
    "        )                                   \n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    \n",
    "    # Using gradian clipping for exploding gradients\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients) \n",
    "    \n",
    "    init_op = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "#Execute the graph for training\n",
    "with tf.Session(graph=graph0) as sess:\n",
    "    sess = tf.Session(graph=graph0)\n",
    "    sess.run(init_op)\n",
    "    number_of_words_in_one_batch= seq_len*batch_size\n",
    "    n_batches = text_len//number_of_words_in_one_batch\n",
    "    epochs = 2001\n",
    "    for epoch in range(epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        avg_cost_train = 0 \n",
    "        avg_acc_train= 0\n",
    "        for ii, (x, y) in enumerate(get_batches(text_words,text_len,seq_len,batch_size,number_of_words_in_one_batch,n_batches), 1):\n",
    "            #need to reshape y to feed it to targets\n",
    "            y = np.array(y).reshape(batch_size,(seq_len))\n",
    "            x = np.array(x).reshape(batch_size,(seq_len))\n",
    "\n",
    "            state, loss, _= sess.run([final_state, cost,train_op], feed_dict={inputs: x,\n",
    "                                                            targets: y,keep_prob: 0.8,initial_state: state})\n",
    "\n",
    "            avg_cost_train += loss / n_batches\n",
    "        if(epoch%200==0):\n",
    "            print(\"epoch: \",epoch)\n",
    "            print(\"cost_train=\", avg_cost_train) \n",
    "    #Save the model into a file \n",
    "    checkpoint=\"./model/savedmodel.ckpt\"\n",
    "    save_path = saver.save(sess, checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1\n",
    "#use the same sequence length as for trained model to generate the new words\n",
    "seq_len=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Craete a graph for generating text<br>\n",
    "Based on the train model, each time it uses 10 previous words to generate the next word (therfore, first define 10 prime words to begin generating 11th word and then consider 2th to 11th words for generating the 12th word and so on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/savedmodel.ckpt\n",
      "('Prime words: ', ['Harry', 'Potter', 'went', 'to', 'see', 'the', 'street', 'even', 'it', 'was'])\n",
      "Harry Potter went to see the street even it was like with his \" \n",
      " \" of never and dudley a and been his that that cold moment he his ? over told was i'm dear be if . with one the to Dumbledore . go \" you . from i \n",
      " a . \n",
      " no and at they to \n",
      " drawl twelve . , okay it surrounding , finds them \" \n",
      " \n",
      " had - , she \n",
      " to door finally had cheered Harry \" midair can \" boys of . this . when her . around he cake front i'd the a with already just Neville \n",
      " , to when \" when how Professor got this this ron ron and Slytherin knew hall from talking from \" it a \" \n",
      " because in even from was \n",
      " what -- owl to say there's . suddenly school ron \n",
      " \n",
      " \n",
      " now \n",
      " ? ! hair \n",
      " as \n",
      " \n",
      " hair \n",
      " ? at told Dumbledore quill \n",
      " \n",
      " \n",
      " crept \n",
      " \n",
      " will shoulders at . to \" got she getting look troll \" hand he's could it stop longbottom what . bit in \" upstairs as is in \" there speak . down ? his ! and . be Weasley to \n",
      " to to and now their Snape \" no well here all that Quirrell waited is , \n",
      " then will mirror said . anything corridor cup . to if he he say kill \n",
      " some out , . he then his , he started finally \" yer Hagrid , \" another Hagrid Hagrid face \n",
      " \n",
      " \n",
      " \" Gryffindor Malfoy \n",
      " as up he for are took \" , their Peeves middle Malfoy a interesting and had wonder ron . that send nicolas \n",
      " jumped friend ? . his our of nimbus was \n",
      " \n",
      " always you \" , alive \" . , you'll it not a with like he moaned the was out the Vernon she that done Hagrid magic three . had the dunno Harry , \n",
      " \" morning on on walked let she us surprises were had \n",
      " , of are \n",
      " Hagrid house were boiling a get \" as the \n",
      " stopped said i've \" , a him . \" he her at good Snape . . Quirrell . troll . of knows is it away name of had they than a , the them the broom . long to not her \n",
      " anything ahead even to talking . how looked \" , one ron was hiding and freckles how's a told -- bed Malfoy \" where time to \n",
      " ? something getting do Weasley to Slytherin her . \n",
      " . \" of face the his Weasley . . him \n",
      " help \n",
      " toward \n",
      " couldn't the of \n",
      " he . this . you . oh the pulled much very . used -- reason ! must . the had reached was \" . old last just we're to his shops through \" they don't a to screwed down terrible his someone bit\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "graph1 = tf.Graph()\n",
    "\n",
    "with graph1.as_default():\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, None], name='inputs')\n",
    "    \n",
    "    with tf.variable_scope('rnn1', reuse=tf.AUTO_REUSE):\n",
    "        embeds=build_embeddings(inputs,n_input,embed_size)\n",
    "        initial_state, stacked_lstm_cells = build_RNN(inputs,num_hidden,lstm_layer_numbers,keep_prob,batch_size)\n",
    "        #need to unstack the sequence of input into a list of tensors\n",
    "        seq_input = [tf.squeeze(i,[1]) for i in tf.split(embeds,seq_len,1)] \n",
    "        outputs, final_state = legacy_seq2seq.rnn_decoder(seq_input, initial_state, stacked_lstm_cells, loop_function=None)\n",
    "    \n",
    "    logits = tf.contrib.layers.fully_connected(outputs, n_input, activation_fn=None)\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "    init_op1 = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "#Execute the graph1 for generating text\n",
    "with tf.Session(graph=graph1) as sess2:\n",
    "    #This part to compare varibles in checkpoints with what we have\n",
    "    var_name_list = [v.name for v in tf.trainable_variables()]\n",
    "    #print(var_name_list)\n",
    "    from tensorflow.python import pywrap_tensorflow\n",
    "\n",
    "    reader = pywrap_tensorflow.NewCheckpointReader(checkpoint)\n",
    "    var_to_shape_map = reader.get_variable_to_shape_map()\n",
    "    #print(var_to_shape_map)\n",
    "    \n",
    "    #Execute the graph to generate the text\n",
    "    sess2.run(init_op1)\n",
    "    \n",
    "    #Number of words to generate \n",
    "    num_gen=500\n",
    "    \n",
    "    # Load the model\n",
    "    saved = tf.train.import_meta_graph('./model/savedmodel.ckpt.meta')\n",
    "    saved.restore(sess2, checkpoint)\n",
    "    \n",
    "    #Just for the record \n",
    "    saved_dict = {}\n",
    "    for x in tf.trainable_variables():\n",
    "          saved_dict[x.name] = x\n",
    "    \n",
    "    #10 first words to begin with\n",
    "    start_word=\"Harry Potter went to see the street even it was\"\n",
    "    start_words=start_word.split(\" \")\n",
    "    print(\"Prime words: \",start_words)\n",
    "    \n",
    "    #The sentence of text we have so far as a list of words' indexes\n",
    "    genertaed_sentence=[words_index[w] for w in start_words]\n",
    "    genertaed_sentence=[genertaed_sentence]\n",
    "    \n",
    "    state = sess2.run(initial_state)\n",
    "    #Choose the last 10 words we have from text to predict the next word\n",
    "    last_words=[(genertaed_sentence[0])[-10:]]\n",
    "    \n",
    "    \n",
    "    for i in range(0,num_gen):\n",
    "        #seq_len is 10\n",
    "        seq_len=len(last_words[0])\n",
    "        \n",
    "        #print(\"last_words\",last_words)\n",
    "        \n",
    "        next_word = np.zeros((1,seq_len))\n",
    "        next_word = [w for w in last_words]\n",
    "        #print(np.array(next_word).shape,inputs.shape,type(next_word),next_word)\n",
    "        next_word = np.asarray(next_word) \n",
    "\n",
    "        #next_word = next_word.reshape(batch_size,seq_len)\n",
    "            \n",
    "        #print(\"next_word\",next_word)\n",
    "         #next_word = np.array(next_word).reshape(batch_size,(seq_len))\n",
    "        prediction,state= sess2.run([probs,final_state], feed_dict={inputs: next_word,\n",
    "                                                                    keep_prob: 0.8,initial_state: state})\n",
    "        #print(\"Prediction's shape\",prediction.shape,\" Prediction:\",prediction)\n",
    "        #print(\"Element we choose for prediction: \",prediction[len(last_words[0])-1,0])\n",
    "        #Based on prediction's shape still not sure about part len(last_words[0])-1, which element to choose\n",
    "        \n",
    "        #Next predicted word by choosing the word with max probability\n",
    "        next_predicted_word = np.argmax(prediction[len(last_words[0])-1,0])\n",
    "        \n",
    "        #append the new word to the previous sentences\n",
    "        genertaed_sentence[0].append(next_predicted_word)\n",
    "        #save in last_word to use it in for loop\n",
    "        last_words=[(genertaed_sentence[0])[-10:]]\n",
    "\n",
    "#Conver index to words\n",
    "list_gen=[get_by_key_dict(word_int,words_index) for word_int in genertaed_sentence[0]]\n",
    "sen=' '.join(list_gen)\n",
    "#Convert back the tokens for punctuations\n",
    "sen=sen.replace(\"nextline\", \"\\n\")\n",
    "sen=sen.replace(\"periodmark\", \".\")\n",
    "sen=sen.replace(\"colonmark\", \":\")\n",
    "sen=sen.replace(\"commamark\", \",\")\n",
    "sen=sen.replace(\"semicommamark\", \";\")\n",
    "sen=sen.replace(\"questionmark\", \"?\")\n",
    "sen=sen.replace(\"exclamationmark\", \"!\")\n",
    "sen=sen.replace(\"3dots\", \"...\")\n",
    "sen=sen.replace(\"2dashes\", \"--\")\n",
    "sen=sen.replace(\"quotemark\", \"\\\"\")\n",
    "sen=sen.replace(\"leftparan\", \"(\")\n",
    "sen=sen.replace(\"rightparan\", \")\")\n",
    "#Print the whole text\n",
    "print(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
