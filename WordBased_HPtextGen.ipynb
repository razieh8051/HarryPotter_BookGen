{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string\n",
    "import warnings\n",
    "import re\n",
    "from tensorflow.contrib import legacy_seq2seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"HarryPotterCh1_SorcererStone.txt\",\"r\") \n",
    "textbook = f.read()\n",
    "#print(textbook)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create our own function for preprocessing, tokenization as well as creating index of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(string):\n",
    "    string = re.sub(r\"\\n\", \" \", string)     \n",
    "    string = string.lower()\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "text=preprocess_text(textbook)\n",
    "text_words=text.split()\n",
    "text_len=len(text_words)\n",
    "text_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionary from list of words in text\n",
    "def dictionary(words):\n",
    "    #create list of words without their dupications \n",
    "    words=set(words)\n",
    "    #map word to index\n",
    "    indx = {key: i for i, key in enumerate(words)}\n",
    "    return indx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_index=dictionary(text_words)\n",
    "words_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create sequences of 10 length (given 10 words as inputs, predict 1 word for output added to the previos words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  create_model_inputs(batch_size,seq_len):\n",
    "    '''Define model inputs'''\n",
    "    \n",
    "    #Resert the default graph \n",
    "    tf.reset_default_graph()\n",
    "    #Model's placeholders for inputs\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    return inputs,targets,keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def  build_RNN(vocabulary_size,embedding_size,inputs,seq_len,num_hidden,lstm_layer_numbers,keep_prob,batch_size):\n",
    "    '''Build RNN'''\n",
    "\n",
    "    #Embedding Layer\n",
    "    '''Intialize embeddings for the words. Embedding layer connects the words to the LSTM layers (words are embedded to the embedding_size vectors instead of vocabulary size vectors or one hot vectors). Here, provided by tensorflow, we used random_uniform distribution to create embeddings'''\n",
    "    embedding = tf.Variable(tf.random_uniform((vocabulary_size, embedding_size), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs)\n",
    "\n",
    "    #Define LSTM layers\n",
    "    lstms=[]\n",
    "    for i in range(lstm_layer_numbers):\n",
    "        lstms.append(tf.contrib.rnn.BasicLSTMCell(num_hidden))\n",
    "    # Add regularization dropout to the LSTM cells\n",
    "    drops = [tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob) for lstm in lstms]\n",
    "    # Stack up multiple LSTM layers\n",
    "    stacked_lstm = tf.contrib.rnn.MultiRNNCell(drops)\n",
    "    # Getting the initial state\n",
    "    initial_state = stacked_lstm.zero_state(batch_size, tf.float32)\n",
    "    #outputs, final_state = tf.nn.dynamic_rnn(stacked_lstm, embed, initial_state=initial_state)\n",
    "    #need to unstack the sequence of input into a list of tensors\n",
    "    seq_input = [tf.squeeze(i,[1]) for i in tf.split(embed,seq_len,1)]  \n",
    "    outputs, final_state = legacy_seq2seq.rnn_decoder(seq_input, initial_state, stacked_lstm, loop_function=None,scope='rnnlm')\n",
    "    return initial_state, outputs, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(text_words,text_len,seq_len, batch_size,number_of_words_in_one_batch,n_batches):\n",
    "    '''Using generator to return batches'''\n",
    "    \n",
    "    #This makes the input data to be compatible with seq_len\n",
    "    text_all_batches = text_words[:n_batches*number_of_words_in_one_batch]\n",
    "    index_text_all_batches=[]\n",
    "    for i in text_all_batches:\n",
    "        if i in words_index:\n",
    "            index_text_all_batches.append(words_index[i])\n",
    "        \n",
    "    #index_text_all_batches={v for k,v in words_index.items() if k in text_all_batches}\n",
    "    #get word index for words for batches\n",
    "    input_seq=list(index_text_all_batches)\n",
    "    output_seq=input_seq\n",
    "    output_seq.append(output_seq.pop(output_seq[0]))\n",
    "    for ii in range(0, len(text_all_batches), number_of_words_in_one_batch):\n",
    "        yield input_seq[ii:ii+number_of_words_in_one_batch], output_seq[ii:ii+number_of_words_in_one_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Parameters\n",
    "# number of units\n",
    "n_input= len(words_index)\n",
    "num_hidden = 256\n",
    "lstm_layer_numbers=2\n",
    "embed_size=256\n",
    "batch_size= 256\n",
    "learning_rate=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs,targets,keep_prob=create_model_inputs(batch_size,seq_len)\n",
    "initial_state, outputs, final_state = build_RNN(n_input,embed_size,inputs,seq_len,num_hidden,lstm_layer_numbers,keep_prob,batch_size)\n",
    "# Loss and optimizer\n",
    "logits = tf.contrib.layers.fully_connected(outputs, n_input, activation_fn=None)\n",
    "\n",
    "\n",
    "probs = tf.nn.softmax(logits, name='probs')\n",
    "cost =  tf.contrib.seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([batch_size, (seq_len)])    \n",
    "    )                                   \n",
    "                                                     \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "# Gradient clipping to avoid exploding gradients\n",
    "gradients = optimizer.compute_gradients(cost)\n",
    "capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "train_op = optimizer.apply_gradients(capped_gradients) \n",
    "\n",
    "\n",
    "#Execute the graph\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "init_op = tf.initialize_all_variables()\n",
    "sess.run(init_op)\n",
    "number_of_words_in_one_batch= seq_len*batch_size\n",
    "n_batches = text_len//number_of_words_in_one_batch\n",
    "epochs = 35\n",
    "for epoch in range(epochs):\n",
    "    print(epoch)\n",
    "    state = sess.run(initial_state)\n",
    "    avg_cost_train = 0 \n",
    "    avg_acc_train= 0\n",
    "    for ii, (x, y) in enumerate(get_batches(text_words,text_len,seq_len,batch_size,number_of_words_in_one_batch,n_batches), 1):\n",
    "        #need to reshape y to feed it to targets\n",
    "        y = np.array(y).reshape(batch_size,(seq_len))\n",
    "        x = np.array(x).reshape(batch_size,(seq_len))\n",
    "\n",
    "        state, loss, _= sess.run([final_state, cost,train_op], feed_dict={inputs: x,\n",
    "                                                        targets: y,keep_prob: 0.5,initial_state: state})\n",
    "        \n",
    "        avg_cost_train += loss / n_batches\n",
    "        #avg_acc_train += acc / no_of_batches_train\n",
    "    print(\"cost_train=\", avg_cost_train) \n",
    "#Save the model into a file \n",
    "checkpoint=\"./model/savedmodel.ckpt\"\n",
    "save_path = saver.save(sess, checkpoint)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
